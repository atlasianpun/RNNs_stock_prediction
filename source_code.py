# -*- coding: utf-8 -*-
"""Source_Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NH_J2KSsoeZV1ha5_QExwTm3hWrTzemh
"""

import pandas as pd
import numpy as np
import math
from sklearn.model_selection import train_test_split

# Read CSV file
def read_csv_file(file_path):
    df = pd.read_csv(file_path)
    return df

# Example usage:
file_path = 'https://raw.githubusercontent.com/blueed9696/dataset_ml_final/main/TXN_seasonally_adjusted.csv'  # Replace 'data.csv' with the path to your CSV file
df = read_csv_file(file_path)

# Separate the 'Open' column as the target variable (y)
X = df['Open']

# Split the data into train and test sets
# Adjust test_size and random_state according to your needs
train_x, test_x = train_test_split(X, test_size=0.2, shuffle=False)

# Convert train_x and test_x to numpy arrays
train_x = train_x.values
test_x = test_x.values

# Define the sequence length
sequence_lengths = 50  # Adjust as needed

# Create the training data
X_train = []
y_train = []
X_test = []
y_test = []

print(len(df))
for i in range(sequence_length, len(train_x)):
    X_train.append(train_x[i-sequence_length:i])
    y_train.append(train_x[i])

for i in range(sequence_length, len(test_x)):
    X_test.append(test_x[i-sequence_length:i])
    y_test.append(test_x[i])

# Convert the lists to numpy arrays
X_train = np.array(X_train)
y_train = np.array(y_train)

X_test = np.array(X_test)
y_test = np.array(y_test)

print(X_train.shape)
print(y_train.shape)

import numpy as np

def clip(x, high=10, low=-10):
    # Clip each element of the matrix
    clipped_matrix = np.clip(x, low, high)
    return clipped_matrix

class SimpleRNN:
    def __init__(self, input_size, hidden_size, output_size, activation='tanh'):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Initialize weights
        self.Wxh = np.random.randn(hidden_size, input_size) * 0.1  # Input to hidden
        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.1  # Hidden to hidden
        self.Why = np.random.randn(output_size, hidden_size) * 0.1  # Hidden to output

        # Choose activation function
        if activation == 'tanh':
            self.activation = np.tanh
            self.activation_derivative = lambda x: 1 - x ** 2
        elif activation == 'sigmoid':
            self.activation = lambda x: 1 / (1 + np.exp(-x))
            self.activation_derivative = lambda x: x * (1 - x)
        elif activation == 'relu':
            self.activation = lambda x: np.maximum(0, x)
            self.activation_derivative = lambda x: np.where(x > 0, 1, 0)
        else:
            raise ValueError("Invalid activation function. Choose from 'tanh', 'sigmoid', or 'relu'.")

    def forward(self, inputs):
        # Initialize the hidden state
        h = np.zeros((self.hidden_size, 1))  # Initial hidden state

        # Store the input data
        self.inputs = inputs

        # Dictionary to store hidden states
        self.hs = {0: h}

        # List to store outputs for each time step
        self.outputs = []

        # Forward pass
        for t, x in enumerate(inputs):
            x = x[:, np.newaxis]

            # Compute the hidden state using the input data and previous hidden state
            h = self.activation(np.dot(self.Wxh, x) + np.dot(self.Whh, h))

            # Store the computed hidden state
            self.hs[t+1] = h

            # Compute the output using the computed hidden state
            output = np.dot(self.Why, h)

            # Store the computed output
            self.outputs.append(output)

        # Convert the list of outputs to numpy array
        return np.array(self.outputs), self.hs

    def backward(self, targets, lr=0.01):
        # Initialize gradients
        dWxh = np.zeros_like(self.Wxh)  # Gradient of the loss with respect to the weights connecting the input to the hidden layer
        dWhh = np.zeros_like(self.Whh)  # Gradient of the loss with respect to the weights connecting the hidden layer to itself
        dWhy = np.zeros_like(self.Why)  # Gradient of the loss with respect to the weights connecting the hidden layer to the output layer
        dhnext = np.zeros_like(self.hs[0])  # Gradient of the loss with respect to the next time step's hidden state

        # Initialize a variable to keep track of the number of data points
        num_data_points = len(targets)

        # Compute loss and gradients
        loss = 0
        for t in reversed(range(len(targets) - 50, len(targets))):

          # Compute the error gradient with respect to the output
          dy = (self.outputs[t] - targets[t])

          # Update gradients for the output weights
          dWhy += np.dot(dy, self.hs[t+1].T)

          # clipping the gradient [-1, 1]
          dWhy = clip(dWhy)

          # Compute the error gradient with respect to the next hidden state
          dh = np.dot(self.Why.T, dy) + dhnext

          # Compute the raw hidden state gradient
          dhraw = self.activation_derivative(self.hs[t+1]) * dh

          # Update gradients for the hidden-to-hidden weights
          dWxh += np.dot(dhraw, self.inputs[t][np.newaxis, :])
          dWhh += np.dot(dhraw, self.hs[t].T)

          dWxh = clip(dWxh)
          dWhh = clip(dWhh)

          # Update the gradient for the next time step's hidden state
          dhnext = np.dot(self.Whh.T, dhraw)

          # Accumulate loss
          loss += np.sum((self.outputs[t] - targets[t]) ** 2*0.5)

        # Calculate the average MSE
        average_mse = loss / num_data_points

        # Update weights using the computed gradients
        self.Wxh -= lr * dWxh
        self.Whh -= lr * dWhh
        self.Why -= lr * dWhy

        return loss

    def train(self, inputs, targets, lr=0.01, epochs=10):
        for epoch in range(epochs):
            self.outputs, hs = self.forward(inputs)
            loss = self.backward(targets, lr)
            if epoch % 10 == 0:
                print(f'>>Epoch {epoch}, Loss: {loss}')

    def predict(self, inputs):
        outputs, _ = self.forward(inputs)
        return outputs

# Define the parameters to test
sequence_lengths = 50
learning_rates = [0.0001, 0.0005, 0.001, 0.01, 0.1]
input_size = 50  # Default input size
hidden_size = 50
output_size = 1

# Define activation functions
activation_functions = ['tanh', 'sigmoid', 'relu']

# Define learning rates
learning_rates = [0.0001, 0.0005, 0.001, 0.01, 0.1]

# Dictionary to store losses for each combination
losses = {}

# Dictionary to store predictions for each combination
all_predictions = {}

# Dictionary to store training and testing MSE and MAPE
mse_mape_results = {}

# Iterate over each combination of activation function and learning rate
for activation_func in activation_functions:
    for lr in learning_rates:
        print(f"Training model with activation function {activation_func} and learning rate {lr}...")

        # Create and train the model
        model = SimpleRNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size, activation=activation_func)
        model.train(X_train, y_train, lr=lr, epochs=4000)

        # Make predictions on test data
        test_predictions = model.predict(X_test)

        # Calculate MSE loss on test data
        test_mse_loss = np.mean((test_predictions - y_test) ** 2)

        # Calculate MAPE loss on test data
        test_mape_loss = np.mean(np.abs((test_predictions - y_test) / y_test)) * 100

        # Store the losses for testing
        losses[(activation_func, lr)] = {'Test MSE': test_mse_loss, 'Test MAPE': test_mape_loss}

        # Store the test predictions
        all_predictions[(activation_func, lr)] = test_predictions

        # Make predictions on training data
        train_predictions = model.predict(X_train)

        # Calculate MSE loss on training data
        train_mse_loss = np.mean((train_predictions - y_train) ** 2)

        # Calculate MAPE loss on training data
        train_mape_loss = np.mean(np.abs((train_predictions - y_train) / y_train)) * 100

        # Store the losses for training
        mse_mape_results[(activation_func, lr)] = {'Train MSE': train_mse_loss, 'Train MAPE': train_mape_loss}

# Plot predictions and loss for each combination of activation function and learning rate
colors = ['r', 'g', 'y', 'c', 'm', 'r', 'g', 'y', 'c', 'm', 'r', 'g', 'y', 'c', 'm']  # Define colors for each learning rate
iter = 0;
for (activation_func, lr), predictions in all_predictions.items():
    loss = losses[(activation_func, lr)]
    mse_loss = loss['Test MSE']

    plt.figure(figsize=(12, 6))

    # Plot predictions
    plt.plot(np.arange(len(predictions)), predictions.squeeze(), label=f'Predictions (LR={lr})', linestyle='-', color=colors[iter])

    # Plot test data
    plt.plot(np.arange(len(y_test)), y_test, label='Test Data', color='b')

    # Add test MSE error to the title
    plt.title(f'Predictions for Test Data (Activation Func={activation_func}, LR={lr})\nTest MSE: {mse_loss:.4f}')

    plt.xlabel('Time')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)

    plt.show()

    iter += 1

# Calculate the train:test split ratio
train_test_ratio = f"8:{2}"

# Open the log file for writing
with open('log_file_10_cases.txt', 'w') as f:
    # Write header
    f.write("Iteration\tActivation Function\tLearning Rate\tHidden Layers\tTrain:Test Split\tTrain MSE\tTrain MAPE\tTest MSE\tTest MAPE\n")

    # Iterate over each combination of activation function and learning rate
    for iteration, ((activation_func, lr), test_predictions) in enumerate(all_predictions.items(), start=1):
        test_loss = losses[(activation_func, lr)]
        test_mse_loss = test_loss['Test MSE']
        test_mape_loss = test_loss['Test MAPE']

        train_loss = mse_mape_results[(activation_func, lr)]
        train_mse_loss = train_loss['Train MSE']
        train_mape_loss = train_loss['Train MAPE']

        # Get the number of hidden layers
        num_hidden_layers = len(model.Whh)

        # Write log entry
        f.write(f"{iteration}\t\t{activation_func}\t\t\t{lr}\t\t{num_hidden_layers}\t\t{train_test_ratio}\t\t{train_mse_loss:.4f}\t{train_mape_loss:.4f}\t{test_mse_loss:.4f}\t{test_mape_loss:.4f}\n")

# Print confirmation message
print("Log file 'log_file_10_cases.txt' has been created.")

